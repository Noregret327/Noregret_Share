## 线性回归与逻辑回归

### 线性回归

#### MSE(cost function)

**线性回归的拟合函数**
$$
f(x)=w^⊤x+b
$$
**cost function(MSE)**
$$
\begin{aligned}
J&=\frac{1}n\sum_{i=1}^n(f(x_i)−y_i)^2\\
 &=\frac{1}n\sum_{i=1}^n(w^Tx_i+b−y_i)^2
\end{aligned}
$$



### 梯度下降

为什么不直接求出导数，然后令导数为0，然后解方程获得极值点的参数值，而要去用梯度下降法来逐步逼近求？

原因是：

* 梯度为0的方程不一定能解出来，但是我们计算梯度的值是可以算的

* 以线性回归为例通过代价函数对参数求导，令其为零，得出参数为：

  $$Θ=(X^TX)^{-1}X^TY$$

  参数的结果给出两个信息，同时也是直接求导不可行的原因：
  X的转置乘以X必须要可逆，也就是X必须可逆，但是实际情况中并不一定都满足这个条件，因此直接求导不可行；假设可逆，那么就需要去求X的转置乘以X这个整体的逆，线性代数中给出了求逆矩阵的方法，是非常复杂的(对计算机来说就是十分消耗性能的)，数据量小时，还可行，一旦数据量大，计算机求矩阵的逆将会是一项非常艰巨的任务，消耗的性能以及时间巨大，而在机器学习中，数据量少者上千，多者上亿；因此直接求导不可行。相较而言，梯度下降算法同样能够实现最优化求解，通过多次迭代使得代价函数收敛，并且使用梯度下降的计算成本很低，所以基于以上两个原因，回归中多数采用梯度下降而不是求导等于零来求参数。



### 正则化线性模型

> 通过限制高次项的系数防止过拟合

​		在机器学习的诸多方法中，假设给定了一个比较小的数据集让我们来做训练，我们常常遇到的问题可能就是**过拟合** (over-fitting) 了，即训练出来的模型可能将数据中隐含的噪声和毫无关系的特征也表征出来。

​		为了避免类似的过拟合问题，一种解决方法是在 (机器学习模型的) 损失函数中加入正则项，比如用 $l_1$ **范数**表示的正则项，只要使得 $l_1$范数的数值尽可能变小，就能够让我们期望的解变成一个**稀疏解** (即解的很多元素为0)。
$$
minf(x)+||x||_1
$$



#### 正则化类别

* L2正则化：
  * 作用：可以使得w都很小，趋近于0，剔除某个特征的影响
  * 优点：越小的参数说明模型越简单，越简单的模型则不容易产生过拟合现象
  * Ridge回归
* L1正则化
  * 作用：可以使得其中一些w为0，删除这个特征的影响
  * LASSO回归



#### 岭回归

> 岭回归是线性回归的正则化版本，在原来线性回归的cost function中添加正则项

​		当样本特征很多，而样本数相对较少时，上式很容易陷入过拟合。为了缓解过拟合问题，可对上式引入正则化项，若使用 L2 范数正则化

岭回归对式（2）加入 L2 正则化，其 cost function 如下：
$$
J=\frac{1}n\sum_{i=1}^n(f(x_i)−y_i)^2+λ||w||{_2^2}
$$



#### Lasso回归

Lasso 回归对式（2）加入 L1 正则化，其 cost function 如下：
$$
J=\frac{1}n\sum_{i=1}^n(f(x_i)−y_i)^2+λ||w||_1
$$


#### 弹性网络

弹性网络在岭回归和Lasso回归中进行了折中，通过**混合比(mix ratio)**进行控制：

* r=0，弹性网络为岭回归
* r=1，弹性网络为Lasso回归

弹性网络的代价函数：
$$
J=\frac{1}n\sum_{i=1}^n(f(x_i)−y_i)^2+rα\sum_{i=1}^n|θ_i|+\frac{1-r}2α\sum_{i=1}^n{θ_i^2}
$$


#### Early Stopping

early stopping是正则化迭代学习的方法之一

其做法是：在验证错误率达到最小值时停止训练



#### 小结

* 常用：岭回归

* 假设只有少部分特征有用：

  * 弹性网络
  * Lasso
  * 一般来说，弹性网络的使用更为广泛，因为在特征维度高于训练样本数，或者特征是强相关的情况下，Lasso回归的表现不太稳定

* api：

  * ```python
    from sklearn.linear_model import Ridge, ElasticNet, Lasso
    ```



### 逻辑回归

> 逻辑回归（Logistic Regression）是机器学习中的一种分类模型，逻辑回归是一种分类算法，虽然名字中带有回归，但是与回归之间存在一定的联系，由于算法的简单喝高效，在实际应用中非常广泛

逻辑回归的输入是线性回归的输出

#### 激活函数（sigmoid函数）

$$
g(θ^Tx)=\frac{1}{1+e^{-θ^Tx}}
$$

* 判断标准
  * 回归的结果输入到sigmoid函数中
  * 输出结果：[0,1]区间的概率值，默认为0.5为阈值



#### 损失函数

逻辑回归的损失，称之为**对数似然损失**，公式如下：

* 分开类别：

$$
cost(h_θ(x),y)=
\begin{cases}
-log(h_θ(x))& \text{if y=1}\\
-log(1-h_θ(x))& \text{if y=0}
\end{cases}
$$



#### 逻辑回归api介绍

```python
import sklearn.linear_model import LogisticRegression

sklearn.linear_model.LogisticRegression(solver='liblinear',penalty='l2',C=1.0)
```

* solver可选参数有：{'liblinear', 'sag', 'saga', 'newton-cg', 'lbfgs'}
  * 默认：'liblinear'，用于优化问题的算法，权限仅限于”one-versus-rest“分类（一或其他）
  * penalty：正则化的选择，主要有两种L1和L2（‘liblinear’ 支持l1和l2，但‘newton-cg’, ‘sag’ 和‘lbfgs’ 只支持l2正则化。）
  * C：正则化系数λ的倒数，必须是大于0的浮点数，与SVM一样，值越小则正则化越强

---

## 决策树

> 决策树（decision tree）是一种依托于策略抉择而建立起来的树。机器学习中，决策树是一个预测模型；他代表的是对象属性与对象值之间的一种映射关系。 树中每个节点表示某个对象，而每个分叉路径则代表的某个可能的属性值，从根节点到叶节点所经历的路径对应一个判定测试序列。决策树可以是二叉树或非二叉树，也可以把他看作是 if-else 规则的集合，也可以认为是在特征空间上的条件概率分布。决策树在机器学习模型领域的特殊之处，在于其信息表示的清晰度。决策树通过训练获得的 “知识”，直接形成层次结构。这种结构以这样的方式保存和展示知识，即使是非专家也可以很容易地理解。

### 信息增益（ID3算法）

> 这里的信息是从信息论的信息，信息论里有一个非常重要的概念——**信息熵**，其中这个“熵”（entropy）是指对复杂系统的刻画，可以理解为信息由不干净到干净所需要丢失的部分

举个例子：男女性和活跃度两个特征，哪个对用户流失影响更大

<img src="https://raw.githubusercontent.com/HXiudi/MK_picture/master/img202303221719964.png" alt="微信图片_20230310085545" style="zoom:150%;" />

![微信图片_20230310085540](https://raw.githubusercontent.com/HXiudi/MK_picture/master/img202303221720908.png)

![微信图片_20230310090052](https://raw.githubusercontent.com/HXiudi/MK_picture/master/img202303221720519.png)

以上可以看出：活跃度的信息增益比性别的信息增益大，也就是说，活跃度对用户流失的影响比性别大。

**但是，信息增益会偏向于选择类别更多的属性**

### 信息增益率（C4.5算法）

> **增益率：**增益比率度量是用前面的增益度量Gain（S, A）和所分离信息度量Splitlnformation(如上例的性别，活跃度等)比值来共同定义的

[一文读懂决策树（上）——信息增益、信息增益率、Gini系数 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/136100996)

### 基尼系数（CART算法）

> **基尼值Gini(D)**：从数据集D中随机抽取两个样本，其类别标记不一致的概率，故Gini(D)值越小，数据D的纯度越高。
>
> **基尼系数Gini_index(D)**：一般选择使划分后基尼系数最小的属性作为最优化分属性

| 序号 | 是否有房 | 婚姻状况 | 年收入 | 是否拖欠贷款 |
| ---- | -------- | -------- | ------ | ------------ |
| 1    | 1        | 单身     | 125    | 0            |
| 2    | 0        | 结婚     | 100    | 0            |
| 3    | 0        | 单身     | 70     | 0            |
| 4    | 1        | 结婚     | 120    | 0            |
| 5    | 0        | 离婚     | 95     | 1            |
| 6    | 0        | 结婚     | 60     | 0            |
| 7    | 1        | 离婚     | 220    | 0            |
| 8    | 0        | 单身     | 85     | 1            |
| 9    | 0        | 结婚     | 75     | 0            |
| 10   | 0        | 单身     | 90     | 1            |

要求：对数据集非类标号属性（是否有房，婚姻状况，年收入）分别计算他们的Gini系数增益，**取Gini系数增益值最大的属性作为决策树的根节点属性。**

* 根节点的Gini系数为：

$$
Gini(是否拖欠贷款)=1-(\frac{3}{10})^2-(\frac{7}{10})^2=0.42
$$

* 根据是否有房来进行划分时，Gini系数增益计算过程为：

  有房Gini：

$$
Gini(左子节点)=1-(\frac{3}{3})^2-(\frac{0}{3})^2=0
$$

​		无房Gini：
$$
Gini(右子节点)=1-(\frac{3}{7})^2-(\frac{4}{7})^2=0.4898
$$
​		是否有房：
$$
\{是否有房\}=0.42-\frac{7}{10}\times0.4898-\frac{3}{10}\times0=0.077
$$

* 根据是否结婚来进行划分时，Gini系数增益计算过程为：

  结婚：
  $$
  Gini(结婚)=1-(\frac{0}{4})^2-(\frac{4}{4})^2=0
  $$
  离婚：
  $$
  Gini(离婚)=1-(\frac{1}{2})^2-(\frac{1}{2})^2=0.5
  $$
  单身：
  $$
  Gini(单身)=1-(\frac{2}{4})^2-(\frac{2}{4})^2=0.5
  $$
  婚姻状况：

  * 结婚

  $$
  \{婚姻状况\}=0.42-\frac{4}{10}\times0-\frac{6}{10}\times[1-(\frac{3}{6})^2-(\frac{3}{6})^2]=0.12
  $$

  * 单身
    $$
    \{婚姻状况\}=0.42-\frac{4}{10}\times0.5-\frac{6}{10}\times[1-(\frac{1}{6})^2-(\frac{5}{6})^2]=0.053
    $$

  * 离婚

  $$
  \{婚姻状况\}=0.42-\frac{2}{10}\times0.5-\frac{8}{10}\times[1-(\frac{2}{8})^2-(\frac{6}{8})^2]=0.02
  $$

* 

* 根据年收入来划分，Gini系数增益计算过程为：

对于年收入属性为数值型属性，首先要对数据按升序排序，然后按照从小到大以此用相邻值的中间值作为分隔将样本划分为两组，求出各自的Gini系数增益

### 三种算法的对比

|              | ID3                                                          | C4.5                                                         | CART                                                         |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 分叉情况     | 多叉树                                                       | 多叉树                                                       | 二叉树                                                       |
| 应用类型     | 分类树                                                       | 分类树                                                       | 分类和回归树                                                 |
| 特征选择方式 | 信息增益                                                     | 信息增益比                                                   | 分类树：Gini<br />回归树：平方误差                           |
| 特点         | 只能对描述属性为离散型属性的数据集构造决策树，缺点是倾向于选择取值较多的属性 | 可以处理连续数值型属性，采用一种后剪枝的方法，对于缺失值的处理，缺点是C4.5只适合于能够驻留于内存的数据集 | 可以处理连续型属性，也可以处理离散属性，是信息增益的简化版本 |

### 剪枝cart

#### 为什么要剪枝

决策树容易发生过拟合，通过剪枝减少节点总数或是深度，可以有效避免过拟合现象的发生。

#### 预剪枝

* 每个结点所包含的最小样本数量，例如10，则该结点总样本数小于10，则不再分
* 指定树的高度或者深度，例如树的最大深度为4
* 指定结点的熵小于某个值，不在划分。随着树的增长，在训练样集上的精度是单调上升的，然而在独立的测试样例的精度是先上升后下降

#### 后剪枝

后剪枝，在已生成过拟合的决策树上进行剪枝，可以得到简化版的剪枝决策树

### 决策树算法api

```python
class sklearn.tree.DecisionTreeClassifier(criterion='gini, max_depth=None,random_state=None)
```

* criterion
  * 特征选择标准
  * “gini"或者"entropy”，前者代表基尼系数，后者代表信息增益。一默认"gini"，即CART算法
* min_samples_split
  * 内部节点再划分所需最小样本数
  * 这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。例如有10万样本，建立决策树时，选择min_samples_split=10。可以作为参考。
* min_samples_leaf
  * 叶子节点最少样本数
  * 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。之前的10万样本项目使用min_samples_leaf的值为5，仅供参考。
* max_depth
  * 决策树最大深度
  * 决策树的最大深度，默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间
* random state
  * 随机数种子

### 决策树例子

在jupyter notebook的泰坦尼克号幸存者例子

### 决策树可视化

* sklearn.tree.export_graphviz() 该函数能够导出DOT格式
  * tree.export_graphviz(estimator, out_file="tree.dot",feature_names=[","])

将生成的tree复制到以下的网址，生成决策树

[Webgraphviz](http://webgraphviz.com/)

### 总结

* 优点：简单的理解和解释，树木🌳可视化
* 缺点：决策树学习者不能很好推广数据的过于复杂的树，容易产生过拟合
* 改进：
  * 减枝cart算法
  * 随机森林（集成学习的一种）

## 集成学习

> 集成学习中的boosting和Bagging
>
> boosting解决的是欠拟合问题（boosting逐步增强学习）
>
> Bagging解决的是过拟合问题（Bagging采样学习集成）

### Bagging

Bagging的集成过程如下：

* 采样
  * 从所有样本里面，采样一部分
* 学习
  * 训练弱学习器
* 集成
  * 使用平权投票

Bagging + 决策树/线性回归/逻辑回归/深度学习...=bagging集成学习方法

Bagging优点：

* 均可在原有算法上提高2%左右的泛化正确率
* 简单、方便、通用

### 随机森林+Bagging

> 随机森林 = Bagging + 决策树

随机森林构造过程的关键步骤（用N来表示训练用例（样本）的个数，M表示特征数目）：

* 一次随机选出一个样本，有放回的抽样，重复N次（有可能出现重复的样本）
* 随机选出m个特征，m<<M，建立决策树

### 随机森林api

```python
sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None,bootstrap=True, random_state=None, min_samples_split=2)
```

* n_estimators: integer, optional (default=10)森林里的树木数量120,200,300,500,800,1200
* criterion: string,可选(default="gini")分割特征的测量方法
* max_depth: integer或None,可选（默认=无）树的最大深度5,8,15,25,30
* max_features="auto",每个决策树的最大特征数量
  * If"auto", then max_features=sqrt(n_features).
  * If "sqrt", then max_features=sqrt(n_features)(same as "auto").
  * If"log2", then max_features=log2(n_features).
  * If None, then max_features=n_features.
* bootstrap: boolean, optional(default=True)是否在构建树时使用放回抽样
* min_samples_split:节点划分最少样本数
* min_samples_leaf:叶子节点的最小样本数

超参数：n_estimator, max_depth, min_samples_split, min_samples_leaf

### Boosting

> 将训练注意力集中在错误数据，随着学习的积累从弱到强

实现过程：

* 初始化训练数据权重，初始权重是相等的
* 通过学习器，计算错误率
* 计算这个学习器的投票权重
* 对每个样本重新赋权
* 重复前面四步
* 对构建后的最后学习器进行加权投票

### Bagging 和Boosting的对比

* 数据方面：
  * Bagging：对数据进行采样训练
  * Boosting：根据前一轮学习结果调整数据的重要性
* 投票方面：
  * Bagging：所有学习器平权投票
  * Boosting：对学习器进行加权投票
* 学习顺序：
  * Bagging：学习并行，每个学习器没有依赖关系
  * Boosting：学习串行，学习有先后顺序
* 主要作用：
  * Bagging：提高泛化能力（解决过拟合，也可以说降低方差）
  * Boosting：提高训练精度（解决欠拟合，也可以说降低偏差）

## 聚类

### 聚类算法在现实中的应用

* 用户画像，广告推荐
* 基于位置信息的商业推送
* 图像分割，降维，识别

### 聚类算法的概念

是一种典型的**无监督**学习算法，主要将相似的样本自动归到一个类别当中

在聚类算法中，根据样本划分到不同类别中，对于不同的相似度计算方法，会得到不同的聚类结果，常用的相似度计算方法有欧氏距离法

与分类算法的区别在于：分类算法属于监督的学习算法

### 聚类算法api

* sklearn.cluster.KMeans(n_clusters=8)
  * 参数：
    * n_clusters：开始聚类中心数量
      * 整型，缺省值=8，生成的聚类数，即产生的质心（centroids）数
  * 方法：
    * estimator.fit(x)
    * estimator.predict(x)
    * estimator.fit_predict(x)
      * 计算聚类中心并预测每个样本属于哪个类别，相当于先调用fit后调用predict

### 聚类算法实现流程

k-means其实包含两层内容：

* k -- 选几个中心点
* means -- 均值计算

流程：

* 随机设置k个特征空间内的点作为初始的聚类中心
* 对于其他每个点计算到 k个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别
* 接着对标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值）
* 如果计算得出的新中心点与原中心点一样（质心不再移动），那么结束，否则重新进行第二部过程

注意：

* 由于每次都要计算所有样本与每个质心之间的相似度，在大规模的数据集上，K-Means算法收敛速度较慢

### K-Means优缺点

**优点：**

1.原理简单（靠近中心点），实现容易

2.聚类效果中上（依赖K的选择）

3.空间复杂度o(N),时间复杂度o(IKN)

N为样本点个数，K为中心点个数，I为选代次数

**缺点：**

1.对离群点，噪声敏感（中心点易偏移）

2很难发现大小差别很大的及进行增量计算

3.结果不一定是全局最优，只能保证局部最优（与K的个数及初值选取有关）



---

## 分类评估方法

### 精确率与召回率

#### 混淆矩阵

在分类任务下，预测结果与正确标记之间存在四种不同的组合，构成混淆矩阵（适用于多分类）

要解释清楚精确率和召回率，得先解释混淆矩阵，二分类问题的混淆矩阵由 4 个数构成。

假阴性（FN）： 算法预测为负例（N），实际上是正例（P）的个数，即算法预测错了（False）；

真阴性（TN）：算法预测为负例（N），实际上也是负例（N）的个数，即算法预测对了（True）；

真阳性（TP）：算法预测为正例（P），实际上是负例（N）的个数，即算法预测错了（False）；

假阳性（FP）：算法预测为正例（P），实际上也是正例（P）的个数，即算法预测对了（True）。

混淆矩阵定义如下：

![混淆矩阵](https://raw.githubusercontent.com/HXiudi/MK_picture/master/img202303221719075.jpg)

#### 准确率

> 查的准不准

$$
precision=\frac{TP}{TP+FP}
$$

精准率就是”预测为正例的那些数据里预测正确的数据个数“

#### 召回率

> 查的全不全

$$
recall=\frac{TP}{TP+FN}
$$

召回率就是”真实为正例的那些数据里预测正确的数据个数“

#### 分类评估报告api

```python
sklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None)
```

* y_true：目标真实值
* y_pred：估计器预测目标值
* labels：指定类别对应数字
* target_names：目标类别名称
* return：每个类别精确率和召回率

### ROC曲线和AUC值

#### TPR与FPR

* TPR=TP/(TP+FN)：所有真实类别为1中，预测类别为真的比例
* FPR=FP/(FP+TN)：所有真实类别为0中，预测类别为假的比例

#### ROC曲线

ROC曲线的横轴就是FPRate，纵轴就是TPRate，当两者相等时，代表不论真实类别为1或0的样本，分类器预测1的概率都是相等的，此时AUC为0.5

**我们想要的效果是TPR越大越好**

#### AUC值

* AUC的概率意义是随机抽取一堆正负样本，正样本得分大于负样本的概率
* AUC的最小值为0.5，最大值为1，取值越高越好
* AUC=1，完美分类器，不管取什么阈值，都可以做出完美预测，不存在
* 0.5<AUC<1，优于随机猜测，这个分类器妥善设定阈值，能有预测意义

> 最终AUC的范围在[0.5, 1]之间，越接近1越好

#### 总结

* AUC只能用来评价二分类
* AUC非常适合评价样本不平衡中的分类器性能

## 模型评估

### 误差平方和（SSE）

![微信截图_20230322160029](https://raw.githubusercontent.com/HXiudi/MK_picture/master/img202303221651348.png)

初始点的选择非常重要，否则效果很差

### "肘"方法（Elbow method）— K值确定

（还没学到，后面学）



### 总结

* SSE

  误差平方和，值越小越好

* 肘部法

  下降率突然变缓即可认为最佳的k值

* SC系数

  取值为[-1，1]，其值越大越好

* CH系数

  分数s高，则说明聚类效果越好，用尽量少的类别聚类尽量多的样本，调试获得较好的聚类效果

### 交叉验证集

```python
from sklearn.model_selection import cross_val_score
scores = cross_val_score(模型,数据,标签,cv=10,scoring="neg_mean_squared_error")
rmse_scores = np.sqrt(-scores)
```

**交叉验证**是用来观察模型的稳定性的一种方法，我们将数据划分为n份，依次使用其中一份作为测试集，其他n-1份作为训练集，多次计算模型的精确性来评估模型的平均准确程度。训练集和测试集的划分会干扰模型的结果，因此用交叉验证n次的结果求出的平均值，是对模型效果的一个更好的度量

**虽然均方误差永远为正，但是sklearn当中使用均方误差作为评判标准时，却是计算”负均方误 差“（neg_mean_squared_error）**。这是因为sklearn在计算模型评估指标的时候，会考虑指标本身的性质，均 方误差本身是一种误差，所以被sklearn划分为模型的一种损失(loss)，因此在sklearn当中，都以负数表示。真正的 均方误差MSE的数值，其实就是neg_mean_squared_error去掉负号的数字。

## 特征工程

### 特征提取

> 将任意数据（文本或图像）转换为可用于机器学习的数字特征

* 特征提取分类：
  * 字典特征提取（特征离散化）
  * 文本特征提取
  * 图像特征提取（深度学习）

#### 特征提取API

```python 
sklearn.feature_extraction
# Tf-idf
sklearn.feature_extraction.text.TfidVectorizer
```

#### Tf-idf文本特征提取

> 作用：用以评估一个词对于一个文件集或一个语料库中的其中一份文件的重要程度

* 主要思想：某个词语短语在一篇文章出现的概率高，且在其他文章很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来作为分类
* 重要性：分类机器学习算法进行文章分类中前期数据处理方式

### 特征降维

> 降维是指在某些限定条件下，降低随机变量特征的个数，得到一组不相关主变量的过程

#### 方法

* Fitter（过滤式）：探究特征本身特点、特征与特征和目标值之间关联
  * 方差选择法：低方差特征过滤
  * 相关系数
* Embedded（嵌入式）：算法自动选择特征（特征与目标值之间的关联）
  * 决策树：信息熵，信息增益
  * 正则化：L1,L2
  * 深度学习：卷积等

#### 低方差特征过滤

> 删除低方差的一些特征，前面讲过方差的意义，再结合方差的大小来考虑这个方式的角度

* 特征方差小：某个特征大多样本的值比较相近
* 特征方差大：某个特征很多样本的值都有差别

**API**:

* sklearn.feature_selection.VarianceThreshold(threshold=0.0)
  * 删除所有低方差特征
  * Variance.fit_transform(X)
    * X: numpy array 格式的数据[n_samples,n_features]
    * 返回值：训练集差异低于threshold的特征将被删除，默认值是保留所有非零方差的特征，即删除所有样本中具有相同值的特征。

#### 相关系数

**皮尔逊相关系数**：反映变量之间相关关系密切程度的统计指标。
$$
r = \frac { n \sum\limits  { xy } - \sum\limits  { x } \sum\limits{ y } } { \sqrt { n \sum\limits{ x } ^ { 2 } -  ( \Sigma  { x } ) ^ { 2 } } \sqrt { n \Sigma y ^ { 2 } - ( \Sigma { y } ) ^ { 2 } } }
$$
**斯皮尔曼相关系数**：
$$
R a n k I C = 1 - \frac { 6 \sum\limits   d _ {i} ^ { 2 } } { n ( n ^ { 2 } - 1 ) }
$$
相关系数的特性：

相关系数的值介于-1与+1之间，即-1≤ r ≤+1。其性质如下：

* 当r>0时，表示两变量正相关，r<0时，两变量为负相关
* 当r=1时，表示两变量为完全相关，当r=0时，表示两变量间无相关关系
* 当0<r<1时，表示两变量存在一定程度的相关。且r越接近1，两变量间线性关系越密切；r越接近于0，表示两变量的线性相关越弱
* 一般可按三级划分：|r|<0.4为低度相关；0.4≤|r|<0.7为显著性相关；0.7≤|r|<1为高度线性相关

**API**:

* from scipy.stats import pearsonr,spearmanr
  * x：(N,) array_like
  * y：(N,) array_like Returns：(Pearson's correlation coefficient p-value)

#### 主成分分析

> **高维数据转化为低维数据**的过程，在此过程中可能会**舍弃原有数据**，**创造新的变量**
>
> 在降维的过程中，保留尽可能多的信息

**API**:

* sklearn.decomposition.PCA(n_components=None)
  * 将数据分解为较低维数空间
  * n_components:
    * 小数：表示保留百分之多少的信息
    * 整数：表示到多少维数

---

## 模型保存和加载

api：

```python
sklearn.externals import joblib
```

保存：joblib.dump(model, 'test.pkl')

加载：model = joblib.load('test.pkl')

注意：1.保存文件后缀名：‘’**.pkl“

​			2.加载模型需要通过一个变量来进行接收
